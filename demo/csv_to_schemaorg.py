#!/usr/bin/env python
#-*- mode: python -*-
# csv_to_schemaorg.py

'''
Demo of Versa Pipeline. Converts a CSV with book info into Schema.org

You might first want to be familar with dc_to_schemaorg.py, which has
more comments on basics and doesn't bother with details such as
 command line

python csv_to_schemaorg.py

https://schema.org/Book
'''

import sys
import warnings
from pathlib import Path

import plac # Cmdline processing tool

from amara3 import iri

from versa import ORIGIN, RELATIONSHIP, TARGET
from versa import I, VERSA_BASEIRI, VTYPE_REL, VLABEL_REL
from versa import util
from versa.driver import memory
from versa.reader.csv_polyglot import parse
from versa.writer import md as md
from versa.pipeline import *
from versa.contrib.datachefids import idgen as default_idgen

BOOK_NS = I('https://example.org/')
IMPLICIT_NS = I('http://example.org/vocab/')
SCH_NS = I('https://schema.org/')


from versa.pipeline import *

FINGERPRINT_RULES = {
    # Fingerprint DC book by ISBN & output resource will be a SCH Book
    IMPLICIT_NS('Book'): materialize(SCH_NS('Book'),
                        unique=[
                            (SCH_NS('isbn'), follow(IMPLICIT_NS('identifier'))),
                        ]
    )
}


# Data transformation rules. In general this is some sort of link from an
# Input pattern being matched to output generated by Versa pipeline actions

# In this case we use a dict of expected relationships from fingerprinted
# resources dict values are the action function that updates the output model
# by acting on the provided context (in this case just the triggered
# relationship in the input model)

DC_TO_SCH_RULES = {
    IMPLICIT_NS('title'): link(rel=SCH_NS('name')),
    IMPLICIT_NS('creator'): materialize(SCH_NS('Person'),
                          unique=[
                              #(SCH_NS('name'), attr(IMPLICIT_NS('name'))),
                              (SCH_NS('name'), attr('name')),
                              (SCH_NS('birthDate'), attr(IMPLICIT_NS('date'))),
                          ],
                          links=[
                              #(SCH_NS('name'), attr(IMPLICIT_NS('name'))),
                              (SCH_NS('name'), attr('name')),
                              (SCH_NS('birthDate'), attr(IMPLICIT_NS('date'))),
                          ]
    ),
}


LABELIZE_RULES = {
    # Labels come from input model's DC name rels
    SCH_NS('Book'): follow(SCH_NS('name'))
}


# Just use Python's built-in string.format()
# Could also use e.g. Jinja
VLITERATE_TEMPLATE = '''\
# @docheader

* @iri:
    * @base: https://example.org/
    * @property: http://example.org/vocab/

# /{ISBN} [http://example.org/vocab/Book]

* title: {Title}
* creator:
    * name: {Author}
    * date: {Author_date}
* publisher:
    * name: {Publisher}
    * date: {Pub_date}
* identifier: {ISBN}
    * type: isbn
'''

# Initialize the pipeline
ppl = definition()
# idgen is a function that generates IDs from sets of hashable fields
# If you wanted to customize the pipeline more than adding a few data members,
# you'd subclass
ppl.idgen = default_idgen

# Versa pipelines are often organized into stages, a basic example being
# fingerprinting and transform. Fingerprinting takes each record and gives it a
# (presumably) unique output ID. Transform takes a fuller view of input data
# And generates the entire output

# The bodies of phases themselves are not declarative; they're imperative and
# generally rely on side-effects (in the state of the pipeline definition instance)
# TODO: Explore a more semantically declarative basis for defining phases as well (arm's length from the Python)

@ppl.stage
def fingerprint(ppl):
    '''
    Generates fingerprints from the source model

    Result of the fingerprinting phase is that the output model shows
    the presence of each resource of primary interest expected to result
    from the transformation, with minimal detail such as the resource type
    '''
    print('BOOM!')
    # Apply a common fingerprinting strategy using rules defined above
    new_rids = ppl.fingerprint_helper(FINGERPRINT_RULES)

    # In real code following lines could be simplified to: return bool(new_rids)
    if not new_rids:
        # Nothing found to process, so ret val set to False
        # This will abort pipeline processing of this input & move on to the next, if any
        return False

    # ret val True so pipeline run will continue for this input
    return True


@ppl.stage
def transform(ppl):
    '''
    Executes the main transform rules to go from input to output model
    '''
    # Apply a common transform strategy using rules defined above
    # 
    def missed_rel(link):
        '''
        Callback to handle cases where a transform wasn't found to match a link (by relationship) in the input model
        '''
        warnings.warn(f'Unknown, so unhandled link. Origin :{link[ORIGIN]}. Rel: {link[RELATIONSHIP]}')

    new_rids = ppl.transform_by_rel_helper(DC_TO_SCH_RULES, handle_misses=missed_rel)
    return True


@ppl.stage
def labelize(ppl):
    '''
    Executes a utility rule to create labels in output model for new (fingerprinted) resources
    '''
    # XXX Check if there's already a label?
    # Apply a common transform strategy using rules defined above
    def missed_label(origin, type):
        '''
        Callback to handle cases where a transform wasn't found to match a link (by relationship) in the input model
        '''
        warnings.warn(f'No label generated for: {origin}')
    labels = ppl.labelize_helper(LABELIZE_RULES, handle_misses=missed_label)
    return True


# outrdfttl=None, outrdfxml=None, outliblink=None, outliblinkmf=None,
#         limit=-1, logger=None):
@plac.annotations(
    source=("Path to CSV source file", "positional", None, Path),
)
def main(source):
    input_model = memory.connection()
    with open(source) as csvfp:
        parse(csvfp, VLITERATE_TEMPLATE, input_model)
    md.write([input_model], out=sys.stdout)
    output_model = ppl.transform(input_model=input_model)
    print('Resulting record Fingerprints:', ppl.fingerprints)
    print('Low level JSON dump of output data model: ')
    util.jsondump(output_model, sys.stdout)
    print('Versa literate form of output: ')
    md.write([output_model], out=sys.stdout)


if __name__ == '__main__':
    plac.call(main)
