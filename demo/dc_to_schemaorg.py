#!/usr/bin/env python
#-*- mode: python -*-
# dc_to_schemaorg.py

'''
Demo of Versa Pipeline. Converts a Dublin Core model into Schema.org

python dc_to_schemaorg.py 
'''

import sys
# from pathlib import Path

# import plac # Cmdline processing tool

from amara3 import iri

from versa import ORIGIN, RELATIONSHIP, TARGET
from versa import I, VERSA_BASEIRI, VTYPE_REL, VLABEL_REL
from versa import util
from versa.driver import memory
from versa.reader.md import from_markdown
from versa.writer import md as md
from versa.pipeline import *
from versa.contrib.datachefids import idgen as default_idgen

BOOK_NS = I('https://example.org/')
DC_NS = I('http://purl.org/dc/terms/')
SCH_NS = I('https://schema.org/')

# Input data (e.g. as if parced from DC XML)
# see e.g. the MODS https://library.britishcouncil.co.zw/cgi-bin/koha/opac-export.pl?op=export&bib=59705&format=mods
# Oh yes, this is a great example of how poor JSON's expressiveness is vs XML, but done this way as a concession to developer trends

# Abstractly, Versa pipelines operate by maping a set of input entities
# to an output entity, but in practice the input entities are often bundled
# into some sort of record format. We'll use such terminology interchangeably.

INPUT_RECORDS = []
INPUT_RECORDS.append('''\
# @docheader

* @iri:
    * @base: https://example.org/
    * @property: http://purl.org/dc/terms/

# HalfofaYellowSun [http://purl.org/dc/terms/Book]

* title: Half of a Yellow Sun
* creator:
    * name: Chimamanda Ngozi Adichie
    * date: 1977
* publisher:
    * name: Fourth estate
    * date: 2006
    * issuance: monographic
    * place: London
* pages: 448
* description: Set in Nigeria during the 1960s, this novel contains three main characters who get swept up in the violence during these turbulent years. It is about Africa, about the end of colonialism, about class and race, and the ways in which love can complicate these things.
* subject:
    * scheme: lcsh
    * geographic: Nigeria
    * temporal: 1967-1970
    * topic: Civil War, 1967-1970
    * topic: Social aspects
    * topic: Fiction
* identifier: 9780008205249
    * type: isbn
''')


from versa.pipeline import *

FINGERPRINT_RULES = {
    # Fingerprint DC book by ISBN & output resource will be a SCH Book
    DC_NS('Book'): becomes(SCH_NS('Book'),
                        unique=[
                            (VTYPE_REL, SCH_NS('Book')),
                            # XXX Check that foreach will work if there are values for the key
                            # follow() for links with the specified relationship from the context origin
                            (SCH_NS('isbn'), follow(DC_NS('identifier'))),
                        ]
    )
}


# Data transformation rules. In general this is some sort of link from an
# Input pattern being matched to output generated by Versa pipeline actions

# In this case we use a dict of expected relationships from fingerprinted
# resources dict values are the action function that updates the output model
# by acting on the provided context (in this case just the triggered
# relationship in the input model)

DC_TO_SCH_RULES = {
    DC_NS('title'): link(rel=SCH_NS('name')),
    DC_NS('creator'): materialize(SCH_NS('Person'),
                          unique=[
                              (VTYPE_REL, SCH_NS('Person')),
                              #(SCH_NS('name'), attr(DC_NS('name'))),
                              (SCH_NS('name'), attr('name')),
                              (SCH_NS('birthDate'), attr(DC_NS('date'))),
                          ],
                          links=[
                              #(SCH_NS('name'), attr(DC_NS('name'))),
                              (SCH_NS('name'), attr('name')),
                              (SCH_NS('birthDate'), attr(DC_NS('date'))),
                          ]
    ),
}

# Initialize the pipeline
ppl = definition()
# idgen is a function that generates IDs from sets of hashable fields
# If you wanted to customize the pipeline more than adding a few data members,
# you'd subclass
ppl.idgen = default_idgen

# Versa pipelines are often organized into stages, a basic example being
# fingerprinting and transform. Fingerprinting takes each record and gives it a
# (presumably) unique output ID. Transform takes a fuller view of input data
# And generates the entire output

# The bodies of phases themselves are not declarative; they're imperative and
# generally rely on side-effects (in the state of the pipeline definition instance)
# TODO: Explore a more semantically declarative basis for defining phases as well (arm's length from the Python)

@ppl.stage
def fingerprint(ppl):
    '''
    Generates fingerprints from the source model

    Result of the fingerprinting phase is that the output model shows
    the presence of each resource of primary interest expected to result
    from the transformation, with minimal data such as the resource type
    '''
    # Extract book entities from the input model
    # books = list(util.all_origins(ppl.input_model, only_types=[DC_NS('Book')]))

    resources = list(util.all_origins(ppl.input_model))
    for rid in resources:
        for typ in util.resourcetypes(ppl.input_model, rid):
            if typ in FINGERPRINT_RULES:
                rule = FINGERPRINT_RULES[typ]
                link = (rid, VTYPE_REL, typ, {})
                ctx = context(link, ppl.input_model, ppl.output_model)
                out_rid = rule(ctx)
                ppl.fingerprints[rid] = out_rid

    # if not ppl.fingerprints:
        # ret val False so, pipeline run will abort & move on to the next input
        # return False

    # ret val True so pipeline run will continue for this input
    return True


@ppl.stage
def transform(ppl):
    '''
    Executes the main transform rules to go from input to output model
    '''
    for rid in ppl.fingerprints:
        out_rid = ppl.fingerprints[rid]
        # Go over all the links for the resource
        for o, r, t, attribs in ppl.input_model.match(rid):
            rule = DC_TO_SCH_RULES.get(r)
            if rule:
                # At the heart of the Versa pipeline context is a rel from
                # that based on the input rel, but now using the output
                # fingerprint as the origin
                link = (out_rid, r, t, attribs)
                ctx = context(link, ppl.input_model, ppl.output_model)
                rule(ctx)

    return True


if __name__ == '__main__':
    for rec in INPUT_RECORDS:
        input_model = memory.connection()
        from_markdown(rec, input_model)
        output_model = ppl.transform(input_model=input_model)
        print('Resulting record Fingerprints:', ppl.fingerprints)
        print('Versa literate form of output: ')
        md.write([output_model], out=sys.stdout)
        print('Low level JSON dump of output data model: ')
        util.jsondump(output_model, sys.stdout)


    # rowid = interphase['record_fingerprint']
    # isnew, item_id = create_resource_mt(
    #     output_model,
    #     rowtype,
    #     unique(obj),
    #     links(obj),
    #     existing_ids=existing_ids)

    # if isnew:
    #     label = label_maker(obj)
    #     if label:
    #         output_model.add(item_id, RDFS_LABEL, label)

    # if k in rules:
    #     rule = rules[k]
    #     if rule:
    #         link = (item_id, k, v, attribs)
    #         ctx = context(link, input_model, output_model, base=BFZ,
    #                         idgen=idg, existing_ids=existing_ids,
    #                         extras=ctxextras)
    #         rule(ctx)
    # else:
    #     warnings.warn('Unrecognized field name: {}'.format(k))



# @plac.annotations(
#     source=("Path to source file", "positional", None, Path),
#     outversa=("Path to Versa output", "positional", None, Path),
# )
# def main(source, outversa):

# outrdfttl=None, outrdfxml=None, outliblink=None, outliblinkmf=None,
#         limit=-1, logger=None):
