#!/usr/bin/env python
#-*- mode: python -*-
# dc_to_schemaorg.py

'''
Demo of Versa Pipeline. Converts a Dublin Core model into Schema.org

python dc_to_schemaorg.py 
'''

import sys
from pathlib import Path

# import plac # Cmdline processing tool

from amara3 import iri

from versa import ORIGIN, RELATIONSHIP, TARGET
from versa import I, VERSA_BASEIRI, VTYPE_REL, VLABEL_REL
from versa import util
from versa.driver import memory
from versa.reader.md import from_markdown
from versa.writer import md
from versa.pipeline import *
from versa.contrib.datachefids import idgen as default_idgen

BOOK_NS = I('https://example.org/')
DC_NS = I('http://purl.org/dc/terms/')
SCH_NS = I('https://schema.org/')

# Input data (e.g. as if parced from DC XML)
# see e.g. the MODS https://library.britishcouncil.co.zw/cgi-bin/koha/opac-export.pl?op=export&bib=59705&format=mods
# Oh yes, this is a great example of how poor JSON's expressiveness is vs XML, but done this way as a concession to developer trends

# Abstractly, Versa pipelines operate by maping a set of input entities
# to an output entity, but in practice the input entities are often bundled
# into some sort of record format. We'll use such terminology interchangeably.

INPUT_RECORDS = []
INPUT_RECORDS.append('''\
# @docheader

* @iri:
    * @base: https://example.org/
    * @property: http://purl.org/dc/terms/

# HalfofaYellowSun [http://purl.org/dc/terms/Book]

* title: Half of a Yellow Sun
* creator:
    * name: Chimamanda Ngozi Adichie
    * date: 1977
* publisher: Fourth estate
    * date: 2006
    * issuance: monographic
    * place: London
* pages: 448
* description: Set in Nigeria during the 1960s, this novel contains three main characters who get swept up in the violence during these turbulent years. It is about Africa, about the end of colonialism, about class and race, and the ways in which love can complicate these things.
* subject:
    * scheme: lcsh
    * geographic: Nigeria
    * temporal: 1967-1970
    * topic: Civil War, 1967-1970
    * topic: Social aspects
    * topic: Fiction
* identifier: 9780008205249
    * type: isbn
''')


from versa.pipeline import *

# Data transformation rules. In general this is some sort of link from an
# Input pattern being matched to output generated by Versa pipeline actions

# In this case we use a dict of expected relationships from fingerprinted
# resources dict values are the action function that updates the output model
# by acting on the provided context (in this case just the triggered
# relationship in the input model)

DC_TO_SCH_RULES = {
    DC_NS('title'): link(rel=SCH_NS('name')),
    DC_NS('creator'): materialize(SCH_NS('Person'),
                          unique=[
                              (VTYPE_REL, SCH_NS('Person')),
                              (SCH_NS('name'), attr('name')),
                              (SCH_NS('birthDate'), attr(DC_NS('date'))),
                          ],
                          links=[
                              (SCH_NS('name'), attr('name')),
                              (SCH_NS('birthDate'), attr(DC_NS('date'))),
                          ]
    ),
}


# Versa pipelines are divided into phases.
# Usually there are at least two phases: fingerprinting and transform
# Fingerprinting takes each record and gives it a (presumably) unique output ID
# Transform takes the data from the input record and generates the output record

# The bodies of phases themselves are not declarative; they're imperative and generally rely on side-effects
# Future development will include making a declarative basis for defining phases as well

# FIXME: Fingerprinting needs to move into the core of things
# The fingerprint inputs & result are key to stringing the phases together
# Would be based on the logic

def fingerprint_phase(input_model, output_model, **interphase):
    '''
    Generates fingerprints from source records

    Each phase takes the input model and other keyword args
    The is set of keyword args make up what's known as the interphase,
    basically shared state space across phases

    Result of the fingerprinting phase is that the output model shows
    the presence of each resource of primary interest expected to result
    from the transformation, with minimal data such as the resource type
    '''
    # A common item in the interphase is a ref to the source object,
    # which can be used for convenience & optimization
    # In this case we won't use it, but for example if the source record
    # was ab object from JSON you could use this to rapidly access the
    # field needed for fingerprinting rather than going thru Versa query
    # of the input model

    # source_rec = interphase['raw_source']

    # Another common interphase entry is a function that generates IDs
    # from sets of hashable fields
    id_generator = interphase['idgen']
    # Store the record's fingerprint in the interphase
    fingerprints = interphase['fingerprints']  # Should be empty to start

    # Extract all books
    books = list(util.all_origins(input_model, only_types=[DC_NS('Book')]))
    if not books:
        # ret val False so, pipeline run will abort & move on to the next input
        return False

    # One fingerprint for the book itself
    for book in books:
        isbn = next(util.lookup(input_model, book, DC_NS('identifier')))
        fprint = resource_id(BOOK_NS, [('isbn', isbn)])
        fingerprints.append(fprint)
        output_model.add(fprint, VTYPE_REL, SCH_NS('Book'))

    # ret val False so pipeline run will continue for this input
    return True


def transform_phase(input_model, output_model, **interphase):
    '''
    Executes the main transform rules to go from input to output model
    '''
    fingerprints = interphase['fingerprints']
    fprint = fingerprints[0]

    # Until we have a better fingerprinting framework, just make silly assumption there is only one book per record
    books = list(util.all_origins(input_model, only_types=[DC_NS('Book')]))

    for book in books:
        # Go over all the links for the book
        for o, r, t, attribs in input_model.match(book):
            rule = DC_TO_SCH_RULES.get(r)
            if rule:
                # At the heart of the Versa pipeline context is a rel from
                # that based on the input rel, but now using the output
                # fingerprint as the origin
                link = (fprint, r, t, attribs)
                ctx = context(link, input_model, output_model)
                rule(ctx)

    return True


DC_PIPELINE_PHASES = [fingerprint_phase, transform_phase]


if __name__ == '__main__':
    for rec in INPUT_RECORDS:
        input_model = memory.connection()
        from_markdown(rec, input_model)
        output_model, interphase = transform(DC_PIPELINE_PHASES, input_model=input_model, idgen=default_idgen)
        fprint = interphase['fingerprints']
        print('Resulting record Fingerprints:', fprint)
        print('Dump of output data model: ')
        util.jsondump(output_model, sys.stdout)


    # rowid = interphase['record_fingerprint']
    # isnew, item_id = create_resource_mt(
    #     output_model,
    #     rowtype,
    #     unique(obj),
    #     links(obj),
    #     existing_ids=existing_ids)

    # if isnew:
    #     label = label_maker(obj)
    #     if label:
    #         output_model.add(item_id, RDFS_LABEL, label)

    # if k in rules:
    #     rule = rules[k]
    #     if rule:
    #         link = (item_id, k, v, attribs)
    #         ctx = context(link, input_model, output_model, base=BFZ,
    #                         idgen=idg, existing_ids=existing_ids,
    #                         extras=ctxextras)
    #         rule(ctx)
    # else:
    #     warnings.warn('Unrecognized field name: {}'.format(k))



# @plac.annotations(
#     source=("Path to source file", "positional", None, Path),
#     outversa=("Path to Versa output", "positional", None, Path),
# )
# def main(source, outversa):

# outrdfttl=None, outrdfxml=None, outliblink=None, outliblinkmf=None,
#         limit=-1, logger=None):
